{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c036a116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the Wikipedia.org webpage\n",
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the header tags (h1 to h6)\n",
    "headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "\n",
    "# Create a list to store the header tag text\n",
    "header_texts = []\n",
    "\n",
    "# Extract the text from the header tags and store in the list\n",
    "for header in headers:\n",
    "    header_texts.append(header.text.strip())\n",
    "\n",
    "# Create a dataframe from the header tag texts\n",
    "df = pd.DataFrame(header_texts, columns=['Headers'])\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63872fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Name       Term of Office\n",
      "0            Rajendra Prasad(1884–1963)          13 May 1952\n",
      "1   Sarvepalli Radhakrishnan(1888–1975)          13 May 1967\n",
      "2               Zakir Husain(1897–1969)        3 May 1969[†]\n",
      "3    Varahagiri Venkata Giri(1894–1980)         20 July 1969\n",
      "4            M. Hidayatullah(1905–1992)       24 August 1969\n",
      "5    Varahagiri Venkata Giri(1894–1980)       24 August 1974\n",
      "6       Fakhruddin Ali Ahmed(1905–1977)  11 February 1977[†]\n",
      "7                B. D. Jatti(1912–2002)         25 July 1977\n",
      "8       Neelam Sanjiva Reddy(1913–1996)         25 July 1982\n",
      "9           Giani Zail Singh(1916–1994)         25 July 1987\n",
      "10           R. Venkataraman(1910–2009)         25 July 1992\n",
      "11      Shankar Dayal Sharma(1918–1999)         25 July 1997\n",
      "12           K. R. Narayanan(1920–2005)         25 July 2002\n",
      "13      A. P. J. Abdul Kalam(1931–2015)         25 July 2007\n",
      "14      Pratibha Devisingh Patil(1934–)         25 July 2012\n",
      "15          Pranab Mukherjee(1935–2020)         25 July 2017\n",
      "16               Ram Nath Kovind(1945–)         25 July 2022\n",
      "17                Droupadi Murmu(1958–)            Incumbent\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_Presidents_of_India'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the former presidents' information\n",
    "table = soup.find('table', class_='wikitable')\n",
    "\n",
    "# Create empty lists to store the president names and terms of office\n",
    "president_names = []\n",
    "terms_of_office = []\n",
    "\n",
    "# Iterate over each row (excluding the header row)\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    # Find the cells in the current row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Ensure that the row has enough cells to extract the required data\n",
    "    if len(cells) >= 5:\n",
    "        # Extract the president name and term of office from the cells\n",
    "        name = cells[1].text.strip()\n",
    "        term = cells[4].text.strip()\n",
    "        \n",
    "        # Append the data to the respective lists\n",
    "        president_names.append(name)\n",
    "        terms_of_office.append(term)\n",
    "\n",
    "# Create a dataframe from the extracted data\n",
    "df = pd.DataFrame({'Name': president_names, 'Term of Office': terms_of_office})\n",
    "\n",
    "# Display the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "143d697e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams\n",
      "                     Team Matches Points Rating\n",
      "0          Australia\\nAUS      23  2,714    118\n",
      "1           Pakistan\\nPAK      20  2,316    116\n",
      "2              India\\nIND      33  3,807    115\n",
      "3         New Zealand\\nNZ      27  2,806    104\n",
      "4            England\\nENG      24  2,426    101\n",
      "5        South Africa\\nSA      19  1,910    101\n",
      "6         Bangladesh\\nBAN      25  2,451     98\n",
      "7           Sri Lanka\\nSL      28  2,378     85\n",
      "8        Afghanistan\\nAFG      13  1,067     82\n",
      "9         West Indies\\nWI      32  2,201     69\n",
      "10          Zimbabwe\\nZIM      27  1,530     57\n",
      "11          Scotland\\nSCO      30  1,451     48\n",
      "12           Ireland\\nIRE      22    887     40\n",
      "13             Nepal\\nNEP      38  1,340     35\n",
      "14       Netherlands\\nNED      24    816     34\n",
      "15     United States\\nUSA      29    855     29\n",
      "16           Namibia\\nNAM      28    813     29\n",
      "17              Oman\\nOMA      21    522     25\n",
      "18               UAE\\nUAE      39    554     14\n",
      "19  Papua New Guinea\\nPNG      28    106      4\n",
      "\n",
      "Top 10 ODI Batsmen\n",
      "                 Batsman Team                         Rating\n",
      "0             Babar Azam  PAK  898 v West Indies, 10/06/2022\n",
      "1  Rassie van der Dussen   SA      796 v England, 19/07/2022\n",
      "2           Fakhar Zaman  PAK  784 v New Zealand, 29/04/2023\n",
      "3            Imam-ul-Haq  PAK  815 v West Indies, 12/06/2022\n",
      "4           Shubman Gill  IND    738 v Australia, 22/03/2023\n",
      "5           David Warner  AUS     880 v Pakistan, 26/01/2017\n",
      "6           Harry Tector  IRE         725 v Oman, 19/06/2023\n",
      "7            Virat Kohli  IND      911 v England, 12/07/2018\n",
      "8        Quinton de Kock   SA    813 v Sri Lanka, 10/03/2019\n",
      "9           Rohit Sharma  IND    885 v Sri Lanka, 06/07/2019\n",
      "\n",
      "Top 10 ODI Bowlers\n",
      "             Bowler Team                         Rating\n",
      "0    Josh Hazlewood  AUS      733 v England, 26/01/2018\n",
      "1    Mohammed Siraj  IND  736 v New Zealand, 21/01/2023\n",
      "2    Mitchell Starc  AUS  783 v New Zealand, 29/03/2015\n",
      "3        Matt Henry   NZ   691 v Bangladesh, 26/03/2021\n",
      "4       Trent Boult   NZ    775 v Australia, 11/09/2022\n",
      "5        Adam Zampa  AUS      655 v England, 22/11/2022\n",
      "6       Rashid Khan  AFG     806 v Pakistan, 21/09/2018\n",
      "7    Shaheen Afridi  PAK  688 v West Indies, 10/06/2022\n",
      "8  Mujeeb Ur Rahman  AFG      712 v Ireland, 24/01/2021\n",
      "9     Mohammad Nabi  AFG     657 v Zimbabwe, 09/06/2022\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the ICC Cricket website for ODI teams\n",
    "url_teams = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "response_teams = requests.get(url_teams)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup_teams = BeautifulSoup(response_teams.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the top 10 ODI teams\n",
    "table_teams = soup_teams.find('table', class_='table')\n",
    "\n",
    "# Create empty lists to store team names, matches, points, and ratings\n",
    "team_names = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows_teams = table_teams.find_all('tr')\n",
    "\n",
    "# Iterate over each row (excluding the header row)\n",
    "for row in rows_teams[1:]:\n",
    "    # Find the cells in the current row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Extract the required information from the cells\n",
    "    team_name = cells[1].text.strip()\n",
    "    match = cells[2].text.strip()\n",
    "    point = cells[3].text.strip()\n",
    "    rating = cells[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    team_names.append(team_name)\n",
    "    matches.append(match)\n",
    "    points.append(point)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a dataframe for the top 10 ODI teams\n",
    "df_teams = pd.DataFrame({'Team': team_names,\n",
    "                         'Matches': matches,\n",
    "                         'Points': points,\n",
    "                         'Rating': ratings})\n",
    "\n",
    "# Print the dataframe for the top 10 ODI teams\n",
    "print(\"Top 10 ODI Teams\")\n",
    "print(df_teams)\n",
    "print()\n",
    "\n",
    "# Send a GET request to the ICC Cricket website for ODI batsmen\n",
    "url_batsmen = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "response_batsmen = requests.get(url_batsmen)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup_batsmen = BeautifulSoup(response_batsmen.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the top 10 ODI batsmen\n",
    "table_batsmen = soup_batsmen.find('table', class_='table')\n",
    "\n",
    "# Create empty lists to store batsmen names, teams, and ratings\n",
    "batsmen_names = []\n",
    "batsmen_teams = []\n",
    "batsmen_ratings = []\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows_batsmen = table_batsmen.find_all('tr')\n",
    "\n",
    "# Iterate over each row (excluding the header row)\n",
    "for row in rows_batsmen[1:11]:\n",
    "    # Find the cells in the current row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Extract the required information from the cells\n",
    "    batsman_name = cells[1].text.strip()\n",
    "    batsman_team = cells[2].text.strip()\n",
    "    batsman_rating = cells[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    batsmen_names.append(batsman_name)\n",
    "    batsmen_teams.append(batsman_team)\n",
    "    batsmen_ratings.append(batsman_rating)\n",
    "\n",
    "# Create a dataframe for the top 10 ODI batsmen\n",
    "df_batsmen = pd.DataFrame({'Batsman': batsmen_names,\n",
    "                           'Team': batsmen_teams,\n",
    "                           'Rating': batsmen_ratings})\n",
    "\n",
    "# Print the dataframe for the top 10 ODI batsmen\n",
    "print(\"Top 10 ODI Batsmen\")\n",
    "print(df_batsmen)\n",
    "print()\n",
    "\n",
    "# Send a GET request to the ICC Cricket website for ODI bowlers\n",
    "url_bowlers = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "response_bowlers = requests.get(url_bowlers)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup_bowlers = BeautifulSoup(response_bowlers.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the top 10 ODI bowlers\n",
    "table_bowlers = soup_bowlers.find('table', class_='table')\n",
    "\n",
    "# Create empty lists to store bowlers names, teams, and ratings\n",
    "bowlers_names = []\n",
    "bowlers_teams = []\n",
    "bowlers_ratings = []\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows_bowlers = table_bowlers.find_all('tr')\n",
    "\n",
    "# Iterate over each row (excluding the header row)\n",
    "for row in rows_bowlers[1:11]:\n",
    "    # Find the cells in the current row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Extract the required information from the cells\n",
    "    bowler_name = cells[1].text.strip()\n",
    "    bowler_team = cells[2].text.strip()\n",
    "    bowler_rating = cells[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    bowlers_names.append(bowler_name)\n",
    "    bowlers_teams.append(bowler_team)\n",
    "    bowlers_ratings.append(bowler_rating)\n",
    "\n",
    "# Create a dataframe for the top 10 ODI bowlers\n",
    "df_bowlers = pd.DataFrame({'Bowler': bowlers_names,\n",
    "                           'Team': bowlers_teams,\n",
    "                           'Rating': bowlers_ratings})\n",
    "\n",
    "# Print the dataframe for the top 10 ODI bowlers\n",
    "print(\"Top 10 ODI Bowlers\")\n",
    "print(df_bowlers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e73139cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Women's ODI Teams\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,603    172\n",
      "1      England\\nENG      28  3,342    119\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      27  2,820    104\n",
      "4   New Zealand\\nNZ      25  2,553    102\n",
      "5   West Indies\\nWI      27  2,535     94\n",
      "6     Thailand\\nTHA      11    821     75\n",
      "7   Bangladesh\\nBAN      14    977     70\n",
      "8     Pakistan\\nPAK      27  1,678     62\n",
      "9     Sri Lanka\\nSL       9    479     53\n",
      "\n",
      "Top 10 Women's ODI Batting Players\n",
      "                Player Team                          Rating\n",
      "0  Chamari Athapaththu   SL   758 v New Zealand, 03/07/2023\n",
      "1          Beth Mooney  AUS      754 v Pakistan, 21/01/2023\n",
      "2      Laura Wolvaardt   SA     741 v Australia, 22/03/2022\n",
      "3       Natalie Sciver  ENG  755 v South Africa, 15/07/2022\n",
      "4          Meg Lanning  AUS   834 v New Zealand, 24/02/2016\n",
      "5     Harmanpreet Kaur  IND       731 v England, 21/09/2022\n",
      "6      Smriti Mandhana  IND       797 v England, 28/02/2019\n",
      "7         Ellyse Perry  AUS   766 v West Indies, 11/09/2019\n",
      "8      Stafanie Taylor   WI      766 v Pakistan, 07/07/2021\n",
      "9       Tammy Beaumont  ENG         791 v India, 27/06/2021\n",
      "\n",
      "Top 10 Women's ODI All-Rounders\n",
      "             Player Team                          Rating\n",
      "0   Hayley Matthews   WI       392 v Ireland, 26/06/2023\n",
      "1    Natalie Sciver  ENG  395 v South Africa, 11/07/2022\n",
      "2      Ellyse Perry  AUS   548 v West Indies, 11/09/2019\n",
      "3    Marizanne Kapp   SA   419 v West Indies, 10/09/2021\n",
      "4       Amelia Kerr   NZ   356 v West Indies, 25/09/2022\n",
      "5     Deepti Sharma  IND  397 v South Africa, 09/10/2019\n",
      "6  Ashleigh Gardner  AUS      292 v Pakistan, 21/01/2023\n",
      "7     Jess Jonassen  AUS   308 v West Indies, 11/09/2019\n",
      "8     Sophie Devine   NZ     305 v Australia, 05/10/2020\n",
      "9          Nida Dar  PAK     232 v Australia, 21/01/2023\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the ICC Cricket website for women's ODI team rankings\n",
    "url_teams = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "response_teams = requests.get(url_teams)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup_teams = BeautifulSoup(response_teams.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the top 10 women's ODI teams\n",
    "table_teams = soup_teams.find('table', class_='table')\n",
    "\n",
    "# Create empty lists to store team names, matches, points, and ratings\n",
    "team_names = []\n",
    "matches = []\n",
    "points = []\n",
    "ratings = []\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows_teams = table_teams.find_all('tr')\n",
    "\n",
    "# Iterate over each row (excluding the header row)\n",
    "for row in rows_teams[1:11]:\n",
    "    # Find the cells in the current row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Extract the required information from the cells\n",
    "    team_name = cells[1].text.strip()\n",
    "    match = cells[2].text.strip()\n",
    "    point = cells[3].text.strip()\n",
    "    rating = cells[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    team_names.append(team_name)\n",
    "    matches.append(match)\n",
    "    points.append(point)\n",
    "    ratings.append(rating)\n",
    "\n",
    "# Create a dataframe for the top 10 women's ODI teams\n",
    "df_teams = pd.DataFrame({'Team': team_names,\n",
    "                         'Matches': matches,\n",
    "                         'Points': points,\n",
    "                         'Rating': ratings})\n",
    "\n",
    "# Print the dataframe for the top 10 women's ODI teams\n",
    "print(\"Top 10 Women's ODI Teams\")\n",
    "print(df_teams)\n",
    "print()\n",
    "\n",
    "# Send a GET request to the ICC Cricket website for women's ODI batting rankings\n",
    "url_batting = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "response_batting = requests.get(url_batting)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup_batting = BeautifulSoup(response_batting.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the top 10 women's ODI batting players\n",
    "table_batting = soup_batting.find('table', class_='table')\n",
    "\n",
    "# Create empty lists to store player names, teams, and ratings\n",
    "player_names = []\n",
    "player_teams = []\n",
    "player_ratings = []\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows_batting = table_batting.find_all('tr')\n",
    "\n",
    "# Iterate over each row (excluding the header row)\n",
    "for row in rows_batting[1:11]:\n",
    "    # Find the cells in the current row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Extract the required information from the cells\n",
    "    player_name = cells[1].text.strip()\n",
    "    player_team = cells[2].text.strip()\n",
    "    player_rating = cells[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    player_names.append(player_name)\n",
    "    player_teams.append(player_team)\n",
    "    player_ratings.append(player_rating)\n",
    "\n",
    "# Create a dataframe for the top 10 women's ODI batting players\n",
    "df_batting = pd.DataFrame({'Player': player_names,\n",
    "                           'Team': player_teams,\n",
    "                           'Rating': player_ratings})\n",
    "\n",
    "# Print the dataframe for the top 10 women's ODI batting players\n",
    "print(\"Top 10 Women's ODI Batting Players\")\n",
    "print(df_batting)\n",
    "print()\n",
    "\n",
    "# Send a GET request to the ICC Cricket website for women's ODI all-rounder rankings\n",
    "url_allrounder = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "response_allrounder = requests.get(url_allrounder)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup_allrounder = BeautifulSoup(response_allrounder.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the top 10 women's ODI all-rounders\n",
    "table_allrounder = soup_allrounder.find('table', class_='table')\n",
    "\n",
    "# Create empty lists to store player names, teams, and ratings\n",
    "allrounder_names = []\n",
    "allrounder_teams = []\n",
    "allrounder_ratings = []\n",
    "\n",
    "# Find all the rows in the table\n",
    "rows_allrounder = table_allrounder.find_all('tr')\n",
    "\n",
    "# Iterate over each row (excluding the header row)\n",
    "for row in rows_allrounder[1:11]:\n",
    "    # Find the cells in the current row\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    # Extract the required information from the cells\n",
    "    allrounder_name = cells[1].text.strip()\n",
    "    allrounder_team = cells[2].text.strip()\n",
    "    allrounder_rating = cells[4].text.strip()\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    allrounder_names.append(allrounder_name)\n",
    "    allrounder_teams.append(allrounder_team)\n",
    "    allrounder_ratings.append(allrounder_rating)\n",
    "\n",
    "# Create a dataframe for the top 10 women's ODI all-rounders\n",
    "df_allrounder = pd.DataFrame({'Player': allrounder_names,\n",
    "                              'Team': allrounder_teams,\n",
    "                              'Rating': allrounder_ratings})\n",
    "\n",
    "# Print the dataframe for the top 10 women's ODI all-rounders\n",
    "print(\"Top 10 Women's ODI All-Rounders\")\n",
    "print(df_allrounder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf68cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline          Time  \\\n",
      "0   Buy Buy Baby auction is canceled, but buyers a...  13 Hours Ago   \n",
      "1   Elon Musk's Twitter sues top law firm Wachtell...  14 Hours Ago   \n",
      "2   Biogen falls after Alzheimer’s drug approval. ...  14 Hours Ago   \n",
      "3   Justin Bieber's Bored Ape NFT has lost about 9...  14 Hours Ago   \n",
      "4   How the SAVE plan can help student loan borrow...  15 Hours Ago   \n",
      "5   While Americans fear a recession as severe as ...  15 Hours Ago   \n",
      "6   Here's how much the $615 million Powerball jac...  15 Hours Ago   \n",
      "7   10 U.S. counties where prospective buyers need...  15 Hours Ago   \n",
      "8   Two health-care stocks are in the headlines. H...  15 Hours Ago   \n",
      "9   Fed's Goolsbee sees 'golden path' to lower inf...  15 Hours Ago   \n",
      "10  Meta Threads doesn't need the 'negativity' of ...  16 Hours Ago   \n",
      "11  Small businesses may have a hard time finding ...  16 Hours Ago   \n",
      "12  June inflation data will be closely watched by...  16 Hours Ago   \n",
      "13  Sales of Alzheimer’s drug Leqembi may be slow ...  16 Hours Ago   \n",
      "14  These stocks, plus Tesla, are this week's top ...  16 Hours Ago   \n",
      "15  Job market expert: Tech roles are still ‘the m...  16 Hours Ago   \n",
      "16  Stocks making the biggest moves midday: Rivian...  17 Hours Ago   \n",
      "17  Twitter's desktop app sees surge in outages as...  17 Hours Ago   \n",
      "18  Special counsel Jack Smith spent $5.4 million ...  17 Hours Ago   \n",
      "19  What Apple's big bet on India means for the te...  18 Hours Ago   \n",
      "20  The unemployment rate among Black workers incr...  18 Hours Ago   \n",
      "21  These new ETFs give income investors different...  18 Hours Ago   \n",
      "22  We're not giving up on this struggling enterta...  18 Hours Ago   \n",
      "23  Federal student loan repayment is about to cha...  18 Hours Ago   \n",
      "24  RFK Jr. touted bitcoin, but said he wasn't an ...  19 Hours Ago   \n",
      "25  Pay student loans or invest in your 401(k)? Wh...  19 Hours Ago   \n",
      "26  This international stock pro outpaces peers wi...  19 Hours Ago   \n",
      "27  CEO says these 3 A.I. tools can make you more ...  19 Hours Ago   \n",
      "28  This classic recession indicator just hit its ...  20 Hours Ago   \n",
      "29  JPMorgan highlights its top stock picks headin...  20 Hours Ago   \n",
      "\n",
      "                                             NewsLink  \n",
      "0   https://www.cnbc.com/2023/07/07/buy-buy-baby-a...  \n",
      "1   https://www.cnbc.com/2023/07/07/elon-musks-twi...  \n",
      "2                                               /pro/  \n",
      "3   https://www.cnbc.com/2023/07/07/justin-biebers...  \n",
      "4   https://www.cnbc.com/2023/07/07/save-plan-help...  \n",
      "5   https://www.cnbc.com/2023/07/07/americans-fear...  \n",
      "6   https://www.cnbc.com/2023/07/07/heres-how-much...  \n",
      "7   https://www.cnbc.com/2023/07/07/most-expensive...  \n",
      "8                                     /investingclub/  \n",
      "9   https://www.cnbc.com/2023/07/07/feds-goolsbee-...  \n",
      "10  https://www.cnbc.com/2023/07/07/threads-doesnt...  \n",
      "11  https://www.cnbc.com/2023/07/07/small-business...  \n",
      "12                                              /pro/  \n",
      "13  https://www.cnbc.com/2023/07/07/alzheimers-dru...  \n",
      "14                                              /pro/  \n",
      "15  https://www.cnbc.com/2023/07/07/tech-jobs-are-...  \n",
      "16  https://www.cnbc.com/2023/07/07/stocks-making-...  \n",
      "17  https://www.cnbc.com/2023/07/07/twitter-deskto...  \n",
      "18  https://www.cnbc.com/2023/07/07/special-counse...  \n",
      "19  https://www.cnbc.com/2023/07/07/what-apples-bi...  \n",
      "20  https://www.cnbc.com/2023/07/07/the-unemployme...  \n",
      "21                                              /pro/  \n",
      "22                                    /investingclub/  \n",
      "23  https://www.cnbc.com/2023/07/07/federal-studen...  \n",
      "24  https://www.cnbc.com/2023/07/07/rfk-jr-bitcoin...  \n",
      "25  https://www.cnbc.com/2023/07/07/pay-student-lo...  \n",
      "26                                              /pro/  \n",
      "27  https://www.cnbc.com/2023/07/07/three-ai-tools...  \n",
      "28  https://www.cnbc.com/2023/07/07/yield-curve-in...  \n",
      "29                                              /pro/  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the CNBC World website\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the news articles on the page\n",
    "articles = soup.find_all('div', class_='LatestNews-headlineWrapper')\n",
    "#print(articles)\n",
    "# Create empty lists to store the headline, time, and news link\n",
    "headlines = []\n",
    "times = []\n",
    "news_links = []\n",
    "\n",
    "# Iterate over each news article\n",
    "for article in articles:\n",
    "    # Find the headline, time, and news link within each article\n",
    "    headline = article.find('a', class_='LatestNews-headline').text.strip()\n",
    "    time = article.find('time', class_='LatestNews-timestamp').text.strip()\n",
    "    news_link = article.find('a')['href']\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    headlines.append(headline)\n",
    "    times.append(time)\n",
    "    news_links.append(news_link)\n",
    "\n",
    "# Create a dataframe with the extracted data\n",
    "df = pd.DataFrame({'Headline': headlines,\n",
    "                   'Time': times,\n",
    "                   'NewsLink': news_links})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4b57605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                  [Reward is enough]   \n",
      "1   [Explanation in artificial intelligence: Insig...   \n",
      "2            [Creativity and artificial intelligence]   \n",
      "3   [Conflict-based search for optimal multi-agent...   \n",
      "4   [Knowledge graphs as tools for explainable mac...   \n",
      "5   [Law and logic: A review from an argumentation...   \n",
      "6   [Between MDPs and semi-MDPs: A framework for t...   \n",
      "7   [Explaining individual predictions when featur...   \n",
      "8     [Multiple object tracking: A literature review]   \n",
      "9   [A survey of inverse reinforcement learning: C...   \n",
      "10  [Evaluating XAI: A comparison of rule-based an...   \n",
      "11  [Explainable AI tools for legal reasoning abou...   \n",
      "12          [Hard choices in artificial intelligence]   \n",
      "13  [Assessing the communication gap between AI mo...   \n",
      "14  [Explaining black-box classifiers using post-h...   \n",
      "15  [The Hanabi challenge: A new frontier for AI r...   \n",
      "16            [Wrappers for feature subset selection]   \n",
      "17  [Artificial cognition for social human–robot i...   \n",
      "18  [A review of possible effects of cognitive bia...   \n",
      "19  [The multifaceted impact of Ada Lovelace in th...   \n",
      "20  [Robot ethics: Mapping the issues for a mechan...   \n",
      "21        [Reward (Mis)design for autonomous driving]   \n",
      "22  [Planning and acting in partially observable s...   \n",
      "23  [What do we want from Explainable Artificial I...   \n",
      "\n",
      "                                              Authors    Published Date  \\\n",
      "0   [David Silver, Satinder Singh, Doina Precup, R...    [October 2021]   \n",
      "1                                        [Tim Miller]   [February 2019]   \n",
      "2                                 [Margaret A. Boden]     [August 1998]   \n",
      "3   [Guni Sharon, Roni Stern, Ariel Felner, Nathan...   [February 2015]   \n",
      "4                    [Ilaria Tiddi, Stefan Schlobach]    [January 2022]   \n",
      "5                    [Henry Prakken, Giovanni Sartor]    [October 2015]   \n",
      "6   [Richard S. Sutton, Doina Precup, Satinder Singh]     [August 1999]   \n",
      "7         [Kjersti Aas, Martin Jullum, Anders Løland]  [September 2021]   \n",
      "8              [Wenhan Luo, Junliang Xing and 4 more]      [April 2021]   \n",
      "9                     [Saurabh Arora, Prashant Doshi]     [August 2021]   \n",
      "10  [Jasper van der Waa, Elisabeth Nieuwburg, Anit...   [February 2021]   \n",
      "11  [Joe Collenette, Katie Atkinson, Trevor Bench-...      [April 2023]   \n",
      "12  [Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mi...   [November 2021]   \n",
      "13  [Oskar Wysocki, Jessica Katharine Davies and 5...      [March 2023]   \n",
      "14  [Eoin M. Kenny, Courtney Ford, Molly Quinn, Ma...        [May 2021]   \n",
      "15        [Nolan Bard, Jakob N. Foerster and 13 more]      [March 2020]   \n",
      "16                       [Ron Kohavi, George H. John]   [December 1997]   \n",
      "17    [Séverin Lemaignan, Mathieu Warnier and 3 more]       [June 2017]   \n",
      "18  [Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz]       [June 2021]   \n",
      "19                           [Luigia Carlucci Aiello]       [June 2016]   \n",
      "20           [Patrick Lin, Keith Abney, George Bekey]      [April 2011]   \n",
      "21   [W. Bradley Knox, Alessandro Allievi and 3 more]      [March 2023]   \n",
      "22  [Leslie Pack Kaelbling, Michael L. Littman, An...        [May 1998]   \n",
      "23           [Markus Langer, Daniel Oster and 6 more]       [July 2021]   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the Elsevier AI journal website\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the most downloaded articles on the page\n",
    "articles = soup.find_all('article')\n",
    "#print(articles)\n",
    "# # Create empty lists to store the paper title, authors, published date, and paper URL\n",
    "paper_titles = []\n",
    "authors_list = []\n",
    "published_dates = []\n",
    "paper_urls = []\n",
    "\n",
    "# # # Iterate over each article\n",
    "for article in articles:\n",
    "    # Find the paper title, authors, published date, and paper URL within each article\n",
    "    paper_title = article.find('a', class_='sc-5smygv-0 fIXTHm').find_all('h2', class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\")\n",
    "    titles = [title.text.strip() for title in paper_title]\n",
    "    authors = article.find('p', class_='sc-1thf9ly-0 sc-1thf9ly-1 iwnLUR fXmEge').find_all('span', class_='sc-1w3fpd7-0 dnCnAO')\n",
    "    author_names = [author.text.strip() for author in authors]\n",
    "    pub_date = article.find('p', class_='sc-1thf9ly-0 sc-1thf9ly-1 iwnLUR fXmEge').find_all('span', class_='sc-1thf9ly-2 dvggWt')\n",
    "    pub_dates = [dates.text.strip() for dates in pub_date]\n",
    "    paper_url = article.find('a', class_='sc-5smygv-0 fIXTHm')['href']\n",
    "    #print(paper_url)\n",
    "    # Append the data to the respective lists\n",
    "    paper_titles.append(titles)\n",
    "    authors_list.append(author_names)\n",
    "    published_dates.append(pub_dates)\n",
    "    paper_urls.append(paper_url)\n",
    "\n",
    "# # Create a dataframe with the extracted data\n",
    "df = pd.DataFrame({'Paper Title': paper_titles,\n",
    "                   'Authors': authors_list,\n",
    "                   'Published Date': published_dates,\n",
    "                   'Paper URL': paper_urls})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb588b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the dineout.co.in website\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all the restaurant cards on the page\n",
    "cards = soup.find_all('div', class_='restnt-card')\n",
    "\n",
    "# Create empty lists to store the restaurant name, cuisine, location, ratings, and image URL\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# Iterate over each restaurant card\n",
    "for card in cards:\n",
    "    # Find the restaurant name, cuisine, location, ratings, and image URL within each card\n",
    "    restaurant_name = card.find('div', class_='restnt-info').h2.text.strip()\n",
    "    cuisine = card.find('div', class_='restnt-info').p.text.strip()\n",
    "    location = card.find('div', class_='restnt-info').find('span', class_='restnt-loc').text.strip()\n",
    "    rating = card.find('div', class_='rtng-rvwr').find('div', class_='rating').text.strip()\n",
    "    image_url = card.find('div', class_='restnt-thumb').img['data-original']\n",
    "    \n",
    "    # Append the data to the respective lists\n",
    "    restaurant_names.append(restaurant_name)\n",
    "    cuisines.append(cuisine)\n",
    "    locations.append(location)\n",
    "    ratings.append(rating)\n",
    "    image_urls.append(image_url)\n",
    "\n",
    "# Create a dataframe with the extracted data\n",
    "df = pd.DataFrame({'Restaurant Name': restaurant_names,\n",
    "                   'Cuisine': cuisines,\n",
    "                   'Location': locations,\n",
    "                   'Ratings': ratings,\n",
    "                   'Image URL': image_urls})\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a0e81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
